{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "senai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preparando o Ambiente local.\n",
        "No Linux Fedora Crie um usuario Hadoop\n",
        "o comando no terminal do linux\n",
        "\n",
        "#create user hadoop\n",
        "\n"
      ],
      "metadata": {
        "id": "pKO3SmqqFCS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rodando em um linux fedora com hadoop e spark instalado na pasta opt\n",
        "para instalar o spark basta baixar o spark no site da apache colocar ele na pasta opt."
      ],
      "metadata": {
        "id": "m84bBRe1dTwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms86fuo-FBS7",
        "outputId": "57e0e7a8-a5d1-43ab-9b26-a2eac55a04d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 38.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=af7bca9c455d6f967f122be0e38bb22df5abf824ced2f03c8e5e5b9743860f21\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando Depedencias."
      ],
      "metadata": {
        "id": "5PEJNGGMGxml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CmddSTc9fYz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnf install openjdk-8-jdk-headless -qq > /dev/null\n",
        "wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "pip install -q findspark"
      ],
      "metadata": {
        "id": "Xdclf1tuGPR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.2.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "ORCvysS6HC8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando e importando os Dataset\n",
        "\n",
        "```\n",
        "# Isto está formatado como código\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AWNswWczMcXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando um dicionario/ data frame do código usando o pandas."
      ],
      "metadata": {
        "id": "vzLc9zYQt_uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "nome = [\"Project\",\"This\",\"Gutenberg\",\"Is\",\"Of\",\"Copyrigth\",\"Welcome\",\"See\",\"Most\",\"Shakespeare\",\"History\",\"BooK\"]\n",
        "sumario = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "df = pd.DataFrame(list(zip(nome,sumario)), columns = ['Name','Sumário'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbVD4Qk33yo8",
        "outputId": "819d4803-978b-4664-863d-68ad0ed6695c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Name  Sumário\n",
            "0       Project        1\n",
            "1          This        2\n",
            "2     Gutenberg        3\n",
            "3            Is        4\n",
            "4            Of        5\n",
            "5     Copyrigth        6\n",
            "6       Welcome        7\n",
            "7           See        8\n",
            "8          Most        9\n",
            "9   Shakespeare       10\n",
            "10      History       11\n",
            "11         BooK       12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregando o Dataset"
      ],
      "metadata": {
        "id": "m8q5q-jPuRuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df0 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/0\") \n",
        "df1 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/1\")\n",
        "df2 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/2\")\n",
        "df3 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/3\")\n",
        "df4 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/4\")\n",
        "df5 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/5\")\n",
        "df6 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/6\")\n",
        "df7 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/7\")\n",
        "df8 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/8\")\n",
        "df9 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/9\")\n",
        "df10 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/10\")\n",
        "df11 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/11\")\n",
        "df12 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/12\")\n",
        "df13 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/13\")\n",
        "df14 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/14\")\n",
        "df15 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/15\")\n",
        "df16 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/16\")\n",
        "df17 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/17\")\n",
        "df18 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/18\")\n",
        "df19 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/19\")\n",
        "df20 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/20\")\n",
        "df21 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/21\")\n",
        "df22 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/22\")\n",
        "df23 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/23\")\n",
        "df24 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/24\")\n",
        "df25 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/25\")\n",
        "df26 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/26\")\n",
        "df27 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/27\")\n",
        "df28 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/28\")\n",
        "df29 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/29\")\n",
        "df30 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/30\")\n",
        "df31 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/31\")\n",
        "df32 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/32\")\n",
        "df33 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/33\")\n",
        "df34 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/34\")\n",
        "df35 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/35\")\n",
        "df36 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/36\")\n",
        "df37 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/37\")\n",
        "df38 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/38\")\n",
        "df39 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/39\")\n",
        "df40 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/40\")\n",
        "df41 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/41\")\n",
        "df42 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/42\")\n",
        "df43 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/43\")\n",
        "df44 = spark.read.format(\"text\").load(\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/44\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "D0inKnSwMbpj",
        "outputId": "829f61cd-a4c7-494c-a5d8-3d1c2e44e74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2e3f01a5397c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/hadoop/Documentos/Desafio+sesi+senai/dataset/2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformando em arquivos Parquet (para melhor trabalhar)"
      ],
      "metadata": {
        "id": "In53rbhZuctj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.write.parquet(\"/home/hadoop/dataset/0\")\n",
        "df5.write.parquet(\"/home/hadoop/dataset/0\")\n",
        "df13.write.parquet(\"/home/hadoop/dataset/0\")\n",
        "df24.write.parquet(\"/home/hadoop/dataset/0\")\n",
        "df30.write.parquet(\"/home/hadoop/dataset/0\")\n",
        "\n",
        "df1.write.parquet(\"/home/hadoop/dataset/1\")\n",
        "df2.write.parquet(\"/home/hadoop/dataset/1\")\n",
        "df4.write.parquet(\"/home/hadoop/dataset/1\")\n",
        "df23.write.parquet(\"/home/hadoop/dataset/1\")\n",
        "df44.write.parquet(\"/home/hadoop/dataset/1\")\n",
        "\n",
        "df5.write.parquet(\"/home/hadoop/dataset/3\")\n",
        "df9.write.parquet(\"/home/hadoop/dataset/3\")\n",
        "df24.write.parquet(\"/home/hadoop/dataset/3\")\n",
        "df44.write.parquet(\"/home/hadoop/dataset/3\")\n",
        "\n",
        "df2.write.parquet(\"/home/hadoop/dataset/4\")\n",
        "df3.write.parquet(\"/home/hadoop/dataset/4\")\n",
        "\n",
        "df5.write.parquet(\"/home/hadoop/dataset/6\")\n",
        "df44.write.parquet(\"/home/hadoop/dataset/6\")\n",
        "df30.write.parquet(\"/home/hadoop/dataset/7\")\n",
        "df40.write.parquet(\"/home/hadoop/dataset/7\")\n",
        "\n",
        "df1.write.parquet(\"/home/hadoop/dataset/8\")\n",
        "df4.write.parquet(\"/home/hadoop/dataset/8\")\n",
        "df7.write.parquet(\"/home/hadoop/dataset/8\")\n",
        "df35.write.parquet(\"/home/hadoop/dataset/8\")\n",
        "\n",
        "df16.write.parquet(\"/home/hadoop/dataset/9\")\n",
        "df22.write.parquet(\"/home/hadoop/dataset/9\")\n",
        "\n",
        "df13.write.parquet(\"/home/hadoop/dataset/10\")\n",
        "df16.write.parquet(\"/home/hadoop/dataset/10\")\n",
        "df17.write.parquet(\"/home/hadoop/dataset/10\")\n",
        "df28.write.parquet(\"/home/hadoop/dataset/10\")\n",
        "df34.write.parquet(\"/home/hadoop/dataset/10\")\n",
        "\n",
        "df1.write.parquet(\"/home/hadoop/dataset/11\")\n",
        "df9.write.parquet(\"/home/hadoop/dataset/11\")\n",
        "\n",
        "df3.write.parquet(\"/home/hadoop/dataset/5\")\n",
        "df6.write.parquet(\"/home/hadoop/dataset/5\")\n",
        "df9.write.parquet(\"/home/hadoop/dataset/5\")\n",
        "df10.write.parquet(\"/home/hadoop/dataset/5\")\n",
        "df33.write.parquet(\"/home/hadoop/dataset/5\")"
      ],
      "metadata": {
        "id": "nNIWCjP2tvpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lendo os arquivos Parquet"
      ],
      "metadata": {
        "id": "_9vEtJ4mMinj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfParque0t=spark.read.parquet(\"/home/hadoop/dataset/0\")\n",
        "dfParque1t=spark.read.parquet(\"/home/hadoop/dataset/1\")\n",
        "dfParque2t=spark.read.parquet(\"/home/hadoop/dataset/2\")\n",
        "dfParque3t=spark.read.parquet(\"/home/hadoop/dataset/3\")\n",
        "dfParque4t=spark.read.parquet(\"/home/hadoop/dataset/4\")\n",
        "dfParque5t=spark.read.parquet(\"/home/hadoop/dataset/5\")\n",
        "dfParque6t=spark.read.parquet(\"/home/hadoop/dataset/6\")\n",
        "dfParque7t=spark.read.parquet(\"/home/hadoop/dataset/7\")\n",
        "dfParque8t=spark.read.parquet(\"/home/hadoop/dataset/8\")\n",
        "dfParque9t=spark.read.parquet(\"/home/hadoop/dataset/9\")\n",
        "dfParque10t=spark.read.parquet(\"/home/hadoop/dataset/10\")\n",
        "dfParque11t=spark.read.parquet(\"/home/hadoop/dataset/11\")\n"
      ],
      "metadata": {
        "id": "DSt6u9oKEyGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando um cluester"
      ],
      "metadata": {
        "id": "_Ghn8UgyS9jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Sparksession\n",
        "\n",
        "from pyspark.sql import Functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import IntType\n",
        "\n",
        "spark = Sparkession \\\n",
        " .builder \\\n",
        " .appName(\"Processamento dos Dados\") \\\n",
        " .getOrCreate()\n",
        "\n",
        "dfParque0t=spark.read.parquet(\"/home/hadoop/dataset/0\")\n",
        "dfParque1t=spark.read.parquet(\"/home/hadoop/dataset/1\")\n",
        "dfParque2t=spark.read.parquet(\"/home/hadoop/dataset/2\")\n",
        "dfParque3t=spark.read.parquet(\"/home/hadoop/dataset/3\")\n",
        "dfParque4t=spark.read.parquet(\"/home/hadoop/dataset/4\")\n",
        "dfParque5t=spark.read.parquet(\"/home/hadoop/dataset/5\")\n",
        "dfParque6t=spark.read.parquet(\"/home/hadoop/dataset/6\")\n",
        "dfParque7t=spark.read.parquet(\"/home/hadoop/dataset/7\")\n",
        "dfParque8t=spark.read.parquet(\"/home/hadoop/dataset/8\")\n",
        "dfParque9t=spark.read.parquet(\"/home/hadoop/dataset/9\")\n",
        "dfParque10t=spark.read.parquet(\"/home/hadoop/dataset/10\")\n",
        "dfParque11t=spark.read.parquet(\"/home/hadoop/dataset/11\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EvzLKD1rTCay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rodando o ambiente em um cluester local"
      ],
      "metadata": {
        "id": "gC7n4F5gWVo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$SPARK_HOME/bin/spark-submit /home/hadoop/roda.py\n",
        "--class local\n",
        "--master local[4] \\\n",
        "executor-memory 2G \\"
      ],
      "metadata": {
        "id": "qAWtsDSJWOs7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}